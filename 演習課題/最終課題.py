import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import datetime
import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --- è¨­å®šã‚¯ãƒ©ã‚¹ ---
class Config:
    DB_NAME = "tokyu_final_analysis.db"
    URLS = {
        "æ± ä¸Šç·š": "https://ja.wikipedia.org/wiki/%E6%9D%B1%E6%80%A5%E6%B1%A0%E4%B8%8A%E7%B7%9A",
        "æ±æ€¥å¤šæ‘©å·ç·š": "https://ja.wikipedia.org/wiki/%E6%9D%B1%E6%80%A5%E5%A4%9A%E6%91%A9%E5%B7%9D%E7%B7%9A"
    }

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹ ---
class DatabaseManager:
    def __init__(self, db_name):
        self.db_name = db_name
        self.create_table()

    def create_table(self):
        conn = sqlite3.connect(self.db_name)
        cursor = conn.cursor()
        cursor.execute("DROP TABLE IF EXISTS stations")
        cursor.execute("""
            CREATE TABLE stations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                line_name TEXT,
                station_name TEXT,
                interval_km REAL,
                transfers TEXT,
                ward TEXT,
                created_at TEXT
            )
        """)
        conn.commit()
        conn.close()

    def save_data(self, line_name, station_name, interval_km, transfers, ward):
        conn = sqlite3.connect(self.db_name)
        cursor = conn.cursor()
        now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        cursor.execute("""
            INSERT INTO stations (line_name, station_name, interval_km, transfers, ward, created_at)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (line_name, station_name, interval_km, transfers, ward, now))
        conn.commit()
        conn.close()

# --- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œã‚¯ãƒ©ã‚¹ ---
class WikiScraper:
    def __init__(self, db_manager):
        self.db = db_manager
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        self.last_ward = "ä¸æ˜"

    def fetch_and_save(self, line_name, url):
        print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {line_name} ...")
        self.last_ward = "ä¸æ˜"

        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            tables = soup.find_all("table", class_="wikitable")
            target_table = None
            for table in tables:
                headers = [th.text.strip() for th in table.find_all("th")]
                if "é§…é–“ã‚­ãƒ­" in str(headers) or "å–¶æ¥­ã‚­ãƒ­" in str(headers):
                    target_table = table
                    break
            
            if not target_table:
                print("âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return

            rows = target_table.find_all("tr")
            for row in rows:
                cols = row.find_all(["td", "th"])
                if len(cols) < 5: continue
                col_texts = [ele.text.strip() for ele in cols]
                
                try:
                    station_name = ""
                    interval_km = 0.0
                    transfers = "ãªã—"
                    ward = self.last_ward

                    if len(col_texts) > 1:
                        raw_name = col_texts[1]
                        if "é§…" not in raw_name and len(raw_name) < 2: raw_name = col_texts[2]
                        station_name = re.sub(r'\[.*?\]', '', raw_name)
                    
                    if not station_name or station_name == "é§…å" or "ã‚­ãƒ­" in station_name: continue

                    if len(col_texts) > 2:
                        dist_match = re.search(r'([\d\.]+)', col_texts[2])
                        if dist_match: interval_km = float(dist_match.group(1))

                    if len(col_texts) > 4:
                        transfers = re.sub(r'\[.*?\]', '', col_texts[4])

                    if line_name == "æ±æ€¥å¤šæ‘©å·ç·š":
                        ward = "å¤§ç”°åŒº"
                    else:
                        full_text = " ".join(col_texts)
                        ward_match = re.search(r'(åƒä»£ç”°|ä¸­å¤®|æ¸¯|æ–°å®¿|æ–‡äº¬|å°æ±|å¢¨ç”°|æ±Ÿæ±|å“å·|ç›®é»’|å¤§ç”°|ä¸–ç”°è°·|æ¸‹è°·|ä¸­é‡|æ‰ä¸¦|è±Šå³¶|åŒ—|è’å·|æ¿æ©‹|ç·´é¦¬|è¶³ç«‹|è‘›é£¾|æ±Ÿæˆ¸å·)åŒº', full_text)
                        if ward_match:
                            ward = ward_match.group(0)
                            self.last_ward = ward
                        elif self.last_ward != "ä¸æ˜":
                            ward = self.last_ward

                    print(f"  - {station_name} ({ward})")
                    self.db.save_data(line_name, station_name, interval_km, transfers, ward)

                except Exception:
                    continue
            time.sleep(2)
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å¯è¦–åŒ–ã‚¯ãƒ©ã‚¹ ---
def analyze_and_visualize():
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    conn = sqlite3.connect(Config.DB_NAME)
    df = pd.read_sql_query("SELECT * FROM stations", conn)
    conn.close()
    
    if df.empty:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼")
        return

    df['transfer_count'] = df['transfers'].apply(lambda x: x.count("ç·š") if x else 0)

    ridership_data = {
        "äº”åç”°": 95723, "å¤§å´åºƒå°è·¯": 7648, "æˆ¸è¶ŠéŠ€åº§": 19370, "èåŸä¸­å»¶": 12777,
        "æ——ã®å°": 23000, "é•·åŸ": 14880, "æ´—è¶³æ± ": 17037, "çŸ³å·å°": 14877, 
        "é›ªãŒè°·å¤§å¡š": 22365, "å¾¡å¶½å±±": 23587, "ä¹…ãŒåŸ": 15640, "åƒé³¥ç”º": 15216, 
        "æ± ä¸Š": 35181, "è“®æ²¼": 8254, "è’²ç”°": 69179,
        "å¤šæ‘©å·": 3678, "æ²¼éƒ¨": 10289, "éµœã®æœ¨": 19463, "ä¸‹ä¸¸å­": 31438,
        "æ­¦è”µæ–°ç”°": 25710, "çŸ¢å£æ¸¡": 25506
    }
    
    def get_ridership(name):
        for key, value in ridership_data.items():
            if key in name: return value
        return 0
    
    df['ridership'] = df['station_name'].apply(get_ridership)

    print("ğŸ“ˆ ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
    sns.set(style="whitegrid")
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Meiryo', 'Yu Gothic', 'Hiragino Sans', 'TakaoPGothic', 'Noto Sans CJK JP']

    line_colors = {
        "æ± ä¸Šç·š": "#FE7FA7",
        "æ±æ€¥å¤šæ‘©å·ç·š": "#AE0378"
    }

    fig = plt.figure(figsize=(18, 12))
    
    # 1. é§…æ•°æ¯”è¼ƒ
    ax1 = plt.subplot2grid((2, 3), (0, 0))
    sns.countplot(data=df, x='line_name', ax=ax1, palette=line_colors)
    ax1.set_title("â‘  é§…æ•°ã®æ¯”è¼ƒï¼ˆè¦æ¨¡ï¼‰", fontsize=12)

    # 2. æ¥ç¶šæ•°æ¯”è¼ƒ
    ax2 = plt.subplot2grid((2, 3), (0, 1))
    sns.barplot(data=df, x='line_name', y='transfer_count', ax=ax2, palette=line_colors, estimator="mean", errorbar=None)
    ax2.set_title("â‘¡ å¹³å‡æ¥ç¶šè·¯ç·šæ•°ï¼ˆãƒãƒ–æ©Ÿèƒ½ï¼‰", fontsize=12)

    # 3. é§…é–“è·é›¢åˆ†å¸ƒ
    ax3 = plt.subplot2grid((2, 3), (0, 2))
    sns.boxplot(data=df[df['interval_km']>0], x='line_name', y='interval_km', ax=ax3, palette=line_colors)
    sns.swarmplot(data=df[df['interval_km']>0], x='line_name', y='interval_km', ax=ax3, color=".3")
    ax3.set_title("â‘¢ é§…é–“è·é›¢ã®åˆ†å¸ƒï¼ˆãƒã‚¹æ„Ÿè¦šï¼‰", fontsize=12)

    # 4. æ‰€åœ¨åœ°æ§‹æˆï¼ˆå††ã‚°ãƒ©ãƒ•ï¼‰
    ax4 = plt.subplot2grid((2, 3), (1, 0))
    ax4.axis('off')
    lines = df['line_name'].unique()
    pie_positions = [[0.02, 0.1, 0.15, 0.15], [0.20, 0.1, 0.15, 0.15]]

    for i, line in enumerate(lines):
        if i >= len(pie_positions): break
        target_ax = fig.add_axes(pie_positions[i])
        subset = df[df['line_name'] == line]
        ward_counts = subset['ward'].value_counts()
        if not ward_counts.empty:
            target_ax.pie(ward_counts, labels=ward_counts.index, autopct='%1.0f%%', 
                          colors=sns.color_palette("pastel"), textprops={'fontsize': 9})
            target_ax.set_title(f"â‘£ {line}ã®åŒºæ§‹æˆ", fontsize=10, color=line_colors.get(line, "black"), fontweight='bold')

    # 5. å¹³å‡ä¹—é™äººå“¡æ¯”è¼ƒ
    ax5 = plt.subplot2grid((2, 3), (1, 1), colspan=2)
    sns.barplot(data=df, x='line_name', y='ridership', ax=ax5, palette=line_colors, estimator="mean", errorbar=None)
    ax5.set_title("â‘¤ 1é§…ã‚ãŸã‚Šã®å¹³å‡ä¹—é™äººå“¡ï¼ˆè·¯ç·šã®å®ŸåŠ›ï¼‰", fontsize=14, fontweight='bold')
    ax5.set_ylabel("äºº/æ—¥")
    
    for p in ax5.patches:
        height = p.get_height()
        if height > 0:
            ax5.annotate(f'{int(height):,}äºº', (p.get_x() + p.get_width() / 2., height), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

    plt.tight_layout()
    plt.show()
    print("âœ¨ å…¨å·¥ç¨‹å®Œäº†ï¼")

if __name__ == "__main__":
    db = DatabaseManager(Config.DB_NAME)
    scraper = WikiScraper(db)
    for line, url in Config.URLS.items():
        scraper.fetch_and_save(line, url)
    analyze_and_visualize()